{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\all4git\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lib import succeed\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from PIL import Image\n",
    "print('lib import succeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".txt loaded\n",
      "Wall time: 85.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = 'D:\\\\code\\\\tianchi\\\\zhijiang\\\\0920\\\\dataset\\\\DatasetA_train_20180813\\\\DatasetA_train_20180813\\\\'\n",
    "testpath = 'D:\\\\code\\\\tianchi\\\\zhijiang\\\\0920\\\\dataset\\\\DatasetA_test_20180813\\\\DatasetA_test_20180813\\\\DatasetA_test\\\\'\n",
    "path_sub = 'D:\\\\code\\\\tianchi\\\\zhijiang\\\\0920\\\\sub_csv\\\\A\\\\'\n",
    "train = pd.DataFrame(pd.read_table(path + 'train.txt',names = ['file_name','label_code'], encoding='utf-8',sep = '\\t'))\n",
    "attributes = pd.DataFrame(pd.read_table(path + 'attributes_per_class.txt',\n",
    "                                        names = ['label_code','1','2','3','4','5','6','7','8','9','10',\n",
    "                                                '11','12','13','14','15','16','17','18','19','20',\n",
    "                                                '21','22','23','24','25','26','27','28','29','30'],\n",
    "                                        encoding='utf-8',sep = '\\t'))\n",
    "testpd = pd.DataFrame(pd.read_table(testpath + 'image.txt',names = ['file_name','label_code'], encoding='utf-8',sep = '\\t'))\n",
    "\n",
    "print('.txt loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def read_and_decode(filename):\n",
    "    #根据文件名生成一个队列\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)   #返回文件名和文件\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                           'img_raw' : tf.FixedLenFeature([], tf.string),\n",
    "                                       })\n",
    "    try:\n",
    "        img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "        img = tf.reshape(img, [ 64, 64, 3])\n",
    "        #img = tf.cast(img, tf.float32) * (1. / 255) - 0.5\n",
    "        label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "        return img, label\n",
    "    except:\n",
    "        print('This is a 8 bit picture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = read_and_decode(path + 'datasetA_train_RGB')\n",
    "img_batch, label_batch = tf.train.shuffle_batch([img, label],\n",
    "                                                batch_size=1000, capacity=5000,\n",
    "                                                min_after_dequeue=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-9ef7f258bdb3>:116: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#batch_size = 1000\n",
    "#n_batch = dataset_num // batch_size\n",
    "\n",
    "def init_weight(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape,stddev=0.1))#按标准差为0.1的正态分布随机生成w权值\n",
    "\n",
    "def init_bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))#设置常量0.1为偏置值b的预设值\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "    # tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)\n",
    "    # 除去name参数用以指定该操作的name，与方法有关的一共五个参数：\n",
    "\n",
    "    # 第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，\n",
    "    # 具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一\n",
    "\n",
    "    # 第二个参数filter：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，\n",
    "    # 具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，\n",
    "    # 有一个地方需要注意，第三维in_channels，就是参数input的第四维\n",
    "\n",
    "    # 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4\n",
    "\n",
    "    # 第四个参数padding：string类型的量，只能是\"SAME\",\"VALID\"其中之一，这个值决定了不同的卷积方式（后面会介绍）\n",
    "\n",
    "    # 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true\n",
    "\n",
    "    # 结果返回一个Tensor，这个输出，就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    # tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
    "    # 参数是四个，和卷积很类似：\n",
    "    # 第一个参数value：需要池化的输入，一般池化层接在卷积层后面\n",
    "    #所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape\n",
    "\n",
    "    # 第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，\n",
    "    # 因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "\n",
    "    # 第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "    # 池化的步长，默认为1。一般我们设置为2，即不重叠。\n",
    "\n",
    "    # 第四个参数padding：和卷积类似，可以取'VALID' 或者'SAME'\n",
    "\n",
    "    # 返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#输入图片数据占位\n",
    "x = tf.placeholder(tf.float32,[None,64,64,3])#浮点型输入数据，估计是图片的RGB值,\n",
    "                                             #一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape\n",
    "y = tf.placeholder(tf.float32,[None,30])#期望输出的结果是30个特征值，再将其与已知的数据进行比较\n",
    "#可调节dropout率，参数为1则全部丢入网络训练\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#--------卷积第一层开始--------#\n",
    "#设置第一层卷积层的权值与偏置量\n",
    "w_conv1 = init_weight([5,5,3,6])#5*5窗口大小的卷积核，共6个，对RGB三层（3通道）进行卷积，应该得到6*3个64*64的卷积结果\n",
    "b_conv1 = init_bias([6])#因为w、b值初始定义基本类似，可以使用自己定义的函数,每个卷积核一个偏置值\n",
    "\n",
    "#计算第一层的卷积结果,使用ReLU激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x,w_conv1) + b_conv1)#卷积运算方法在TensorFlow中有内置，简化成函数conv2d即可\n",
    "\n",
    "#第一个池化层计算\n",
    "h_pool1 = max_pool(h_conv1)#暂定使用2x2窗口池化，后续可以修改，得到6*3@32*32\n",
    "#--------卷积第一层基本结束--------#\n",
    "\n",
    "#--------卷积第二层开始--------#\n",
    "#设置第二层卷积层的权值与偏置量\n",
    "w_conv2 = init_weight([5,5, 6,12])#5*5窗口大小的卷积核，共12个，对RGB三层（3通道）*6=18进行卷积，应该得到12个32*32的卷积结果\n",
    "b_conv2 = init_bias([12])#因为w、b值初始定义基本类似，可以使用自己定义的函数\n",
    "\n",
    "#计算第二层的卷积结果,使用ReLU激活函数\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,w_conv2) + b_conv2)#卷积运算方法在TensorFlow中有内置，简化成函数conv2d即可\n",
    "\n",
    "#第二个池化层计算\n",
    "h_pool2 = max_pool(h_conv2)#暂定使用2x2窗口池化，后续可以修改,得到16*16*(12*18)\n",
    "#--------卷积第二层基本结束--------#\n",
    "\n",
    "#--------全连接层一开始--------#\n",
    "w_fc1 = init_weight([16*16*12,1024])#16*16*12*18维喂入1024个全连接层第一层\n",
    "b_fc1 = init_bias([1024])#全连接层(full connect)的权值与偏置量，同样可以使用自定义函数简化表达\n",
    "\n",
    "#h_pool2是n*n*h*x形状的，需要将其平滑为一行的tensor\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,16*16*12])#shape参数中的-1应该表示的是自动补齐的意思\n",
    "\n",
    "#第一个全连接计算结果\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,w_fc1) + b_fc1)\n",
    "\n",
    "#使用dropout防止过拟合\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "#--------全连接第一层基本结束--------#\n",
    "\n",
    "#--------隐层开始--------#\n",
    "w_fc_middle = init_weight([1024,256])\n",
    "b_fc_middle = init_bias([256])#全连接层(full connect)的权值与偏置量，同样可以使用自定义函数简化表达\n",
    "h_fc_middle = tf.nn.relu(tf.matmul(h_fc1_drop,w_fc_middle) + b_fc_middle)\n",
    "h_fc_middle_drop = tf.nn.dropout(h_fc_middle,keep_prob)\n",
    "\n",
    "#--------隐层完成--------#\n",
    "\n",
    "\n",
    "#--------全连接第二层/输出层开始--------#\n",
    "w_fc2 = init_weight([256,30])\n",
    "b_fc2 = init_bias([30])#全连接层(full connect)的权值与偏置量，同样可以使用自定义函数简化表达\n",
    "\n",
    "#计算输出层结果，分布域可能挺大，使用sigmoid激活函数处理之\n",
    "logits = tf.matmul(h_fc_middle_drop,w_fc2) + b_fc2\n",
    "#用sigmoid函数处理输出值\n",
    "prediction = tf.sigmoid(logits)\n",
    "#--------全连接第二层/输出层完成--------#\n",
    "\n",
    "#训练网络的策略是使用AdamOptimizer优化器，最小化交叉熵，学习率可以自己设置\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#会话使用例子\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for epoch in range(21):#共训练21个大循环，可自定义\n",
    "#         for batch in range(n_batch):#每次填入batch_size个样本\n",
    "#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "#             sess.run(train_step, feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
    "#         acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0})#输出准确率\n",
    "#         print(\"Iter: \" + str(epoch) + \", acc: \" + str(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#尝试打List[30]形状的标签\n",
    "def batch_30dims(batch_ys):\n",
    "    batch_30 = []\n",
    "    for batch_y in batch_ys:\n",
    "        batch_30.append(become_list( 'ZJL' + str(batch_y) ) )\n",
    "\n",
    "    return batch_30\n",
    "\n",
    "def become_list(label):\n",
    "    batch_30 = attributes.loc[attributes['label_code'] == label]\n",
    "    batch_list = np.array(batch_30).tolist()[0][1:]\n",
    "    return batch_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(pd_pre_one):\n",
    "    pre_value_list = pd_pre_one.iloc[0]\n",
    "    distance = attributes.copy()\n",
    "    for i in range(1,30):\n",
    "        distance[str(i)] = np.square(distance[[str(i)]] - pre_value_list[i])\n",
    "        distance[str(i)].astype('float32')\n",
    "    \n",
    "    tmp = (distance['1'] + distance['2'] +distance['3'] + distance['4'] +distance['5'] + distance['6']+\n",
    "           distance['7'] + distance['8'] +distance['9'] + distance['10'] +distance['11'] + distance['12']+\n",
    "           distance['13'] + distance['14'] +distance['15'] + distance['16'] +distance['17'] + distance['18'] +\n",
    "             distance['19'] + distance['20'] +distance['21'] + distance['22'] +distance['23'] + distance['24'] +\n",
    "             distance['25'] + distance['26'] +distance['27'] + distance['28'] +distance['29'] + distance['30']).tolist()\n",
    "    label_index = tmp.index(min(tmp))\n",
    "    label_pre = distance.iloc[label_index,0]\n",
    "    print(label_pre)\n",
    "    #print(distance.sort_values(by = 'distance').head(5))\n",
    "    return label_pre\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZJL178\n",
      "train over\n",
      "Wall time: 3.44 sINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_UINT8, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](shuffle_batch/random_shuffle_queue, Reshape, Cast)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#使用TensorFlow中的Session进行训练\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())#初始化变量，必不可少的一个准备动作\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord = coord)#个人猜测使用coord协助sess读取\n",
    "    \n",
    "    for epoc in range(25):\n",
    "        for _ in range(200):\n",
    "            batch_xs,batch_ys = sess.run([img_batch,label_batch])\n",
    "            batch_30 = batch_30dims(batch_ys)\n",
    "            sess.run(train_step,feed_dict = {x:batch_xs/255,y:batch_30,keep_prob:0.7})#按train_step的预想训练模型\n",
    "    \n",
    "    \n",
    "    for line in range(testpd.shape[0]):#testpd.shape[0]\n",
    "        img_path = testpath + 'test\\\\' + testpd.iloc[line,0] #每一个图片的地址\n",
    "        img=Image.open(img_path,'r')\n",
    "        size = img.size\n",
    "        img = img.resize((64, 64))\n",
    "        if img.mode == 'RGB':\n",
    "            img_raw=np.array(img).reshape(1,64,64,3)#img.tobytes()#将图片转化为二进制格式\n",
    "            predict_one = sess.run(prediction,feed_dict={x:img_raw/255,keep_prob:0.7})\n",
    "            pd_pre_one = pd.DataFrame(predict_one[0]).T#\n",
    "            pd_pre_one.columns = ['1','2','3','4','5','6','7','8','9','10',\n",
    "                                  '11','12','13','14','15','16','17','18','19','20',\n",
    "                                  '21','22','23','24','25','26','27','28','29','30']\n",
    "            #print(pd_pre_one)\n",
    "            testpd.iloc[line,1] = get_label(pd_pre_one)\n",
    "        else:\n",
    "            testpd.iloc[line,1] = 'ZJL1'\n",
    "    testpd.to_csv(path_sub+\"0911_1sttry.csv\",sep = '\\t',encoding='utf-8',index=None,header=None)\n",
    "       \n",
    "        #current_accuracy = sess.run(accuracy,feed_dict = {})#计算当前验证集中准确率\n",
    "        #print(\"模型当前准确率：\" + \"训练集中：暂时未设置\" + \"验证集中：\" + str(current_accuracy) + \"%\")\n",
    "    print('train over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
